= PREREQUISITES =

Make sure the frozen dataset (gg_frozen.tar.gz) is extracted to
./00-data/. In other words, you should have the following tree
structure:

./00-data/
├── doc.db
├── prompt.txt
├── queries.tsv
└── vec.chroma

Also make sure OPENAI_API_KEY is defined in your environment (see
https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety
)

= PREPILOT README =

The purpose of the prepilot is to do prompt engineering on a small
"dev"-set (if you will) of queries (one per "type" I think), to get to
a point where we have a prompt which gets good interannotator
agreement (>0.6 imo).

If the folder structure looks like above, then the  ./prepilot.py
script should "just work", and do the following:

1. create annotation pool using top-k results from each IR system
2. let llm annotate pool
3. start manual annotation of the same pool using tkinter UI
4. calculate inter-annotator agreement between llm and human evaluations
5. calculate metrics on both manual annotated/llm annotated annotations.

So the prepilot iterative process for you as a prompt engineer should
be:

1. adjust ./data/prompt.txt
2. delete ./01_prepilot_out/01_llm.db to let llm annotate again.
3. run ./prepilot.py
4. (IF FIRST RUN) manual annotate the same pool the llm annotates (see
   annotation instructions below for a UI guide)
5. observe metrics

& then rinse and repeat.

The annotation guidelines below is only a rough first draft, changes are welcome!

= ANNOTATION GUIDELINES

The basic premise around this type of annotation is simple: reflect
the experience of skimming an article one day, and trying to remember
it three months later. Are the retrieved documents _relevant_ to your
idea of what the article was about?

There are three labels: (0) Not relevant, (1) Relevant, (2) Highly
relevant.  What is relevant and what is the distinction between (1)
and (2)? Very basic definition:

    Relevant means that the _majority_ of the query is contained
    _semantically_ in the text/title of the document.

    The distinction between highly relevant and just relevant is
    that for highly relevant documents all parts of the query has to
    be in the document.

I refer to the examples below for how this would look like.

Be lenient, I think, on judgments. In other words, if the query is ---
say --- conspiracy theory and the document is the Wikipedia article
for _The DaVinci Code_, then that is relevant (fight me!)[^1].
Preferably since false positives are of lesser importance than false
negatives for this testset.

These basic rules are problematic, however (which annotation
guidelines aren't?). Some exceptions off the top of my head of edge
cases:

    "conspiracy theory" is one semantic "unit". Technically I wouldn't
    say the Wikipedia article is Highly relevant for this query, as it
    is mainly about a book (which touches on conspiracy theories about
    Christianity). I would therefore label it as "Relevant" not
    "Highly Relevant".

    "father john misty" is one semantic unit, since documents only
    returning "father john" are not relevant, "john misty" the same.
    In other words, named entities ought to be kept together and are
    seen as one unit.

---

[^1]: Well, don't really fight me, it's not I like the book that much
      or anything.

== EXAMPLES:

query:
    justice for snarbo
doc_title:
    chunking_evaluation/chunking_evaluation/evalua...
doc_text:
    = Valkyria Chronicles III = Senjō no Valkyria 3 : Chronicles (
    Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the
    Battlefield 3) , commonly referred to as Valkyria Chronicles III
    outside Japan , is a tactical role ...
label:
    Not relevant

query:
    python library type checker
doc_title:
    Type Checking in Compiler Design
doc_text:
    Type checking is the process of checking and enforcing the
    constraints of types assigned to values in a program. A compiler
    has to check that a source program conforms both to the syntactic
    and semantic rules of the language as well as its type rules. The
    process also helps in limiting the kinds of types that can be used
    in certain contexts, assigning types to values, and then checking
    that these values are used appropriately.
    ...
label:
    Relevant (is about typechecking, but not any mention of python)

query:
    apple email
doc_title:
    From: Steve Jobs. "Great idea, thank you."
doc_text:
    Now that I’ve been retired for a couple of days, I think I can
    finally tell this story of how I was – very briefly – steve@next.com

    And Steve Jobs sent me an email saying “Great idea, thank you."

    Wait, what? What was the great idea?
    ...
label:
    Highly Relevant (Apple->Steve Jobs, Email->email)

= ANNOTATION INSTRUCTIONS

Manual annotation runs a basic tkinter interface for annotation where you can see:

- ID: current document ID
- query: the query used to get the document
- text: the text of the document in question
- Not relevant/Relevant: buttons to label a document
- IDX/TOTAL: a basic counter to indicate current progress.

You can annotate using the keyboard as well, with 0 labelling
something as not relevant, 1 relevant, and 2 highly relevant.

If you accidentally label something incorrectly, just exit the program
(it saves automatically), and remove the latest inputted row using the
following SQL query:

    DELETE FROM annotations WHERE annotation_id = (SELECT MAX(annotation_id) FROM annotations);

Do this from the command line using e.g.

    sqlite3 /path/to/annotations.db \
        "DELETE FROM annotations WHERE annotation_id = (SELECT MAX(annotation_id) FROM annotations);"
